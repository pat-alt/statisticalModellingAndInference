---
title: "Subsampling"
subtitle: "Model selection"
author: "Patrick Altmeyer"
date: "`r format(Sys.Date(), '%d %B, %Y')`"
output: 
  bookdown::html_document2:
    code_folding: hide
    toc: false
    number_sections: false
bibliography: bib.bib
---

```{r setup, include=F}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
# Import helper functions:
helper_functions <- list.files("R")
invisible(
  lapply(
    helper_functions,
    function(i) {
      source(paste0("R/",i))
    }
  )
)
```

The goal of this exercise is to investigate if and how systematic sub-sampling in situations where $n>>p$ can improve Bayesian sequential learning. For simulation exercises we will closely follow Chapter 3 of @bishop2006pattern. The note is structured as follows: to set the stage for the final investigation in the first [section](#bias) I briefly reproduce the example in @bishop2006pattern on the bias-variance trade-off. The following [section](#subsampling) then introduces sub-sampling. The ideas are then brought together in the following section. The final section applies the developed ideas to some example data and concludes.

## Bias-variance tradeoff {#bias}

```{r}
# Generate data: ----
n <- 25
p <- 24
a <- 0
b <- 1
x <- seq(a,b,length.out = n)
y <- sinusoidal(x)
v <- 0.3
```

As our data generating process for $y$ we will consider the sinusoidal function $f(x)=\sin(2\pi x)$ as in @bishop2006pattern. To simulate random samples of $y$ we sample $n$ input values from $x \sim \text{unif}(0,1)$ and introduce some white noise $\varepsilon \sim \mathcal{N}(0,`r v`)$. Figure \@ref(fig:p_sim) shows $y$ along with random draws $y^*_n$.

```{r p_sim, fig.cap="Sinusoidal function and random draws."}
# True data: ----
library(ggplot2)
library(data.table)
dt_true <- data.table(y,x)
pl <- ggplot(data=dt_true, aes(x=x, y=y)) +
  geom_line()
# Simulate data: ----
n_draws <- 3
dt_star <- rbindlist(
  lapply(
    1:n_draws,
    function(x) {
      simulated <- sim_sinusoidal(n=n, sigma = v)
      data.table(y=simulated$y_star, x=simulated$x_star, n=1:n, draw=x)
    }
  )
)
pl +
  geom_point(
    data = dt_star,
    aes(x=x, y=y, colour=factor(draw))
  ) +
  scale_colour_discrete(
    name="Draw:"
  )
```

```{r param-setup}
lambda <- c(
  exp(2.6),
  exp(-0.31),
  exp(-2.4)
)
s <- 0.1
n_draws <- 100
mu <- seq(a,b,length.out = p)
```

As in @bishop2006pattern we will use a Gaussian linear model with Gaussian kernels $\exp(- \frac{(x_k-\mu_p)^{2}}{2s^2})$ to estimate $\hat{y}_k$ from our data $(y_k,x_k)$

$$
\begin{equation} 
\begin{aligned}
&& y|x& =f(x) \sim \mathcal{N} \left( \sum_{j=0}^{p-1} \phi_j(x)\beta_j, v \mathbb{I}_p \right) \\
\end{aligned}
(\#eq:model)
\end{equation}
$$

with $v=`r v`$. We fix the number of kernels $p=`r p`$ (and hence the number of features $M=p+1=`r p+1`$) as well as the spatial scale $s=`r s`$. To vary the complexity of the model we use regularized least-squares and let the regularization parameter $lambda$ vary

$$
\begin{equation} 
\begin{aligned}
&& \hat\beta&=(\lambda I + \Phi^T \Phi)^{-1}\Phi^Ty \\
\end{aligned}
(\#eq:reg-ls)
\end{equation}
$$

where high values of $\lambda$ in \@ref(eq:reg-ls) shrink parameter values towards zero. (Note that a choice $\lambda=0$ corresponds to the OLS estimator.)
As in @bishop2006pattern we proceed as follows for each choice of $\lambda$ and each sample draw:

1. Draw $N=`r n`$ time from $u \sim \text{unif}(`r a`,`r b`)$. 
2. Let $x_k^*=u+\varepsilon$ with $\varepsilon \sim \mathcal{N}(0, `r v`)$.
3. Compute $y_k^*=\sin(2\pi x^*_k)$.
4. Extract features $\Phi_k$ from $x_k^*$ and estimate the parameter vector $\beta_k^*(\Phi_k,y_k,\lambda)$ through regularized least-squares. 
5. Predict $\hat{y}_k^*=\Phi \beta_k^*$.

```{r}
Phi <- cbind(
  rep(1,n),
  sapply(
    1:length(mu),
    function(p) {
      mu_p <- mu[p]
      gauss_kernel(x=x, mu=mu_p, s = s)
    }
  )
)
dt <- rbindlist(
  lapply( # loop - draw K times
    1:n_draws,
    function(k) {
      # Draw:
      simulated <- sim_sinusoidal(n=n, sigma = v)
      y_k <- simulated$y_star
      x_k <- simulated$x_star
      rbindlist(
        lapply( # loop over regularization parameter
          1:length(lambda),
          function(t) {
            # Regularization parameter:
            lambda_t <- lambda[t]
            # Extract features:
            Phi_k <- cbind(
              rep(1,n),
              sapply(
                1:length(mu),
                function(p) {
                  mu_p <- mu[p]
                  gauss_kernel(x=x_k, mu=mu_p, s = s)
                }
              )
            )
            beta_hat <- regularized_ls(Phi_k,y_k,lambda_t) # fit model on (y,x)
            y_hat <- c(Phi %*% beta_hat) # predict from model
            dt <- data.table(value=y_hat,draw=k,lambda=lambda_t,n=1:n,x=x)
            return(dt)
          }
        )
      )
    }
  )
)
dt[,facet_group:="single"]
dt[,colour_group:="estimates"]
# Expected values:
dt_exp = dt[,.(value=mean(value)),by=.(lambda,n,x)]
dt_exp[,facet_group:="aggregate"]
dt_exp[,colour_group:="estimates"]
dt_exp[,draw:=1] # just for aesthetics
# True values:
library(reshape)
dt_true = data.table(expand.grid.df(data.frame(value=y,x=x),data.frame(lambda=lambda)))
dt_true[,facet_group:="aggregate"]
dt_true[,colour_group:="true"]
dt_true[,draw:=2] # just for aesthetics
# Plot data:
dt_plot = rbind(
  dt,
  dt_exp,
  dt_true,
  fill=T
)
```


Recall that for the mean-squared error (MSE) we have 

$$
\begin{equation} 
\begin{aligned}
&& \mathbb{E} \left( (\hat{f}_n(x)-f(x))^2 \right)
&= \text{var} (\hat{f}_n(x)) + \left( \mathbb{E} \left( \hat{f}_n(x) \right) - f(x) \right)^2 \\
\end{aligned}
(\#eq:mse)
\end{equation}
$$

where the first term on the right-hand side corresponds to the variance of our prediction and the second term to its (squared) bias. Applying the above procedure we can construct the familiar picture that demonstrates how increased model complexity increases variance while reducing bias (Figure \@ref(fig:plot-bias-var)).

```{r plot-bias-var, fig.cap="Bias-variance trade-off"}
dt_plot[,log_lambda := log(lambda)]
ggplot(data=dt_plot[draw<=25], aes(y=value, x=x, colour=colour_group, group=draw)) +
  geom_line() +
  facet_grid(
    rows = vars(log_lambda),
    cols = vars(facet_group)
  )
```


## Sub-sampling methods {#subsampling}

Let us now consider the case for sub-sampling: $n >> p$. We continue with the sinusoidal function from above but now look at the case where the number of observations is $n=`r n`$ and the number of features is $p=`r p`$. Suppose we are interested in estimating $\hat\beta_m$ instead of $\hat\beta_m$ where $p\le m=`r m`<<n$ with $m$ freely chosen by us. In practice we may want to do this to avoid high computational costs associated with large $n$. Or perhaps we are only allowed to load to access $m$ observations at a time. The basic algorithm for performing estimating $\hat\beta_m$ is as follows:

1. Subsample with replacement from the data with sampling probability $\{\pi_i\}^n_{i=1}$.
2. Estimate least-squares estimator $\hat\beta_m$ using the subsample. 

But there are at least two questions about this algorithm: firstly, how do we choose $X_m=x^{(1)},...,x^{(m)}$? Secondly, how should we construct $\hat\beta_m$? With respect to the former, a better idea than just randomly selecting $X_m$ might be to choose observations with high influence. We will look at  a few of the different subsampling methods investigated and proposed in @zhu2015optimal, which differ primarily in their choice of subsampling probabilities $\{\pi_i\}^n_{i=1}$:

1. Uniform subsampling (UNIF): $\{\pi_i\}^n_{i=1}=1/n$.
2. Basic leveraging (BLEV): $\{\pi_i\}^n_{i=1}=h_{ii}/ \text{tr}(\mathbb{H})=h_{ii}/p$ where $\mathbb{H}$ is the *hat matrix*.
3. Predictor-length sampling (PL): $\{\pi_i\}^n_{i=1}= ||x_i||/ \sum_{i=1}^{n}||x_i||$ where $||x||$ denotes the $L_2$ norm of $x$.

The PL method is proposed by the authors and shown to scale very well and is good approximation conditional on leverage scores $h_{ii}$ being fairly homogenous.

With respect to the second question @zhu2015optimal investigate both ordinary least-squares (OLS) and weighted least-squares (WLS), where weights simply correspond to subsampling probabilities $\{\pi_i\}^n_{i=1}$. The authors present empirical evidence that OLS is more efficient than WLS in that the mean-squared error (MSE) for predicting $\Phi \beta$ is lower for OLS. Unfortunately though subsampling using OLS is not consistent for non-uniform subsampling methods meaning that the bias cannot be controlled. Given Equation \@ref(eq:mse) this implies that the variance of predictions is higher with WLS and that this effect dominates the effect of relatively higher bias with OLS. In fact this is consistent with the theoretical results presented in @zhu2015optimal (more on this below).

Next we will briefly run through different subsampling and estimation methods ins some more detail and see how they can be implemented in R. In the following [section](#empir) we will then look at how the different approaches perform empirically.

### OLS and WLS

Both OLS and WLS are implemented using QR decompostion. As for OLS this is very easily done in R. Given some feature matrix `Phi` and a corresponding outcome variable `y` we can use `qr.solve(Phi, y)` compute $\hat\beta$. For WLS we need to first weigh observations by their corresponding subsampling probabilities. Following @zhu2015optimal we can construct a weighting matrix $\Omega= \text{diag}\{\pi_i\}^m_{i=1}$ and compute the weighted least-squares estimator as:

$$
\begin{equation} 
\begin{aligned}
&& \hat\beta_m^{WLS}&= \left( \Phi^T \Omega^{-1} \Phi \right)^{-1} \Phi^T\Omega^{-1}y\\
\end{aligned}
(\#eq:wls)
\end{equation}
$$
The derivation for \@ref(eq:wls) is as follows (note that here I'm using the same notation as in @zhu2015optimal where $\Phi$ corresponds to our $\Omega$ and $X$ to our $\Phi$):

![Weighted least-squares](www/wls.png)

In R this can be wrapped up in a simple function:

```{r class.source = "fold-show", code=readLines("R/wls_qr.R"), echo=T}
```

### Uniform leveraging (UNIF)

A simple function for uniform leveraging in R is shown in the code chunk below. Note that to streamline the comparison of the different methods in the following [section](#empir) the function takes an unused argument `weighted=F` which for the other subsampling methods can be used to determine wether OLS or WLS should be used. Of course, with uniform leveraging the weights are all identical so $\hat\beta^{OLS}=\hat\beta^{WLS}$ so the argument is passed to but not evaluated in `UNIF`. (There are likely more elegant ways to do this!) 

```{r class.source = "fold-show", code=readLines("R/UNIF.R"), echo=T}
```

### Basic leveraging (BLEV)

The `UNIF` function can be extended easily to the case with basic leveraging (see code below). Note that in this case the `weighted` argument is evaluated.  

```{r class.source = "fold-show", code=readLines("R/BLEV.R"), echo=T}
```

#### A note on computing leverage scores

Recall that for the *hat matrix* we have

$$
\begin{equation} 
\begin{aligned}
&& \mathbb{H}&=\Phi (\Phi^T \Phi)^{-1}\Phi^T \\
\end{aligned}
(\#eq:hat-mat)
\end{equation}
$$
where the diagonal elements $h_{ii}$ correspond to the leverage scores we're after. Following @zhu2015optimal we will use (compact) singular value decomposition to obtain $\mathbb{H}$ rather than computing \@ref(eq:hat-mat) directly. This has the benefit that there exist exceptionally stable numerical algorithms to compute SVD. (**source**) To see why we can use SVD to obtain $\mathbb{H}$ consider the following:

![From SVD to leverage](www/svd_leverage.png)

Clearly to get $h_{ii}$ we first need to compute $\mathbb{H}$ which in terms of computational costs is of order $\mathcal{O}(np^2)=\max(\mathcal{O}(np^2),\mathcal{O}(p^3))$. The fact that we use all $n$ rows of $\Phi$ to compute leverage scores even though we explicitly stated our goal to only use $m$ observations is a bit of a paradox. This is why fast algorithms that approximate leverage scores have been proposed. We will not look at them specifically here mainly because the method proposed by @zhu2015optimal promises to be computationally even more efficient.

### Predictor-length sampling (PL)

The basic characteristic of PL subsampling - choosing $\{\pi_i\}^n_{i=1}= ||x_i||/ \sum_{j=1}^{n}||x_j||$ - was already introduced above. Again it is very easy to modify the subsampling functions from above to this case:

```{r class.source = "fold-show", code=readLines("R/PL.R")}
```

#### A note on optimal subsampling (OPT)

In fact, this PL subsampling is an approximate version of optimal subsampling (OPT). @zhu2015optimal show that variance $V=\text{var} (\hat{f}_n(x))$ dominates squared bias $\left( \mathbb{E} \left( \hat{f}_n(x) \right) - f(x) \right)^2$. More specifically they show that asymptotically

$$
\begin{equation} 
\begin{aligned}
&& V&\rightarrow m^{-1} \\
\end{aligned}
(\#eq:var-asymp)
\end{equation}
$$

and 

$$
\begin{equation} 
\begin{aligned}
&& \left( \mathbb{E} \left( \hat{f}_n(x) \right) - f(x) \right)^2&\rightarrow (m^{-1})^2 \\
\end{aligned}
(\#eq:var-bias)
\end{equation}
$$

where $m$ is the size of the subsample as before. Given this result minimizing the MSE (Equation \@ref(eq:mse)) with respect to subsampling probabilities $\{\pi_i\}^n_{i=1}$ is equivalent to minimizing $V$. They further show that this minimization problem has the following closed-form solution:

$$
\begin{equation} 
\begin{aligned}
&& \pi_i&= \frac{\sqrt{(1-h_{ii})}||x_i||}{\sum_{j=1}^n\sqrt{(1-h_{jj})}||x_j||}\\
\end{aligned}
(\#eq:opt)
\end{equation}
$$

This still has computational costs of order $\mathcal{O}(np^2)$. But now it becomes clear why PL subsampling is optimal conditional on leverage scores being homogenous:

![From OPT to PL](www/PL.png)

PL subsampling is associated with computational costs of order $\mathcal{O}(np)$ so a great improvement in particular when both $n$ and $p$ are large. 

## Emprirical evaluation {#empir}

### Reproducing @zhu2015optimal

We proceed as in @zhu2015optimal ...

In R we will use the following function for this purpose:

```{r class.source = "fold-show", code=readLines("R/sim_subsampling.R")}
```

```{r zhu-data}
library(expm)
scale_down <- 5
n <- 5000/scale_down
p <- 50/scale_down
matrix_grid <- expand.grid(i=1:p,j=1:p)
Sigma <- matrix(rep(0,p^2),p,p)
for (x in 1:nrow(matrix_grid)) {
  i <- matrix_grid$i[x]
  j <- matrix_grid$j[x]
  Sigma[i,j] <- 2 * (0.8)^(scale_down*abs(i-j))
}
# 1.) Design matrix (as in Zhu et al): ----
GA <- matrix(rnorm(n*p), nrow = n, ncol = p) %*% t(sqrtm(Sigma))
# Gaussian mixture:
gaus_mix <- list(
  gaus_1 = matrix(rnorm(n*p), nrow = n, ncol = p) %*% t(sqrtm(Sigma)),
  gaus_2 = matrix(rnorm(n*p), nrow = n, ncol = p) %*% t(sqrt(p/2) * sqrtm(Sigma))
)
MG <- matrix(rep(0,n*p),n,p)
for (i in 1:nrow(MG)) {
  x <- sample(1:2,1)
  MG[i,] <- gaus_mix[[x]][i,]
}
# Log-Gaussian:
LN <- exp(GA)
# T-distribution:
T1 <- matrix(rt(n*p,1), nrow = n, ncol = p) %*% t(sqrtm(Sigma)) 
# Truncated T:
TT <- T1
TT[TT>p] <- p
TT[TT<(-p)] <- -p
data_sets <- list(
  GA = list(X = GA),
  MG = list(X = MG),
  LN = list(X = LN),
  TT = list(X = TT),
  T1 = list(X = T1)
)
# 2.) Outcome:
data_sets <- lapply(
  data_sets,
  function(i) {
    X <- i[["X"]]
    beta <- c(rep(1,ceiling(0.6*p)),rep(0.1,floor(0.4*p)))
    eps <- rnorm(n=n,mean=0,sd=10)
    y <- X %*% beta + eps
    list(X=X, y=y)
  }
)
# Subsampling methods:
methods <- list(
  "UNIF" = UNIF,
  "BLEV" = BLEV,
  "OPT" = OPT,
  "PL" = PL
)
```

```{r smpl-prob, eval=F}
pgrid <- expand.grid(method = names(methods)[2:4], data=names(data_sets))
smpl_probs <- rbindlist(
  lapply(
    1:nrow(pgrid),
    function(i) {
      method <- as.character(pgrid$method[i])
      data_set <- pgrid$data[i]
      X <- data_sets[[data_set]]$X
      y <- data_sets[[data_set]]$y
      prob <- methods[[method]](X, y, m=NULL, prob_only=T)
      return(data.table(prob=prob, method=method, data=data_set))
    }
  )
)
saveRDS(smpl_probs, "outputs/smpl_probs.rds")
```

```{r load-smpl-probs}
smpl_probs <- readRDS("outputs/smpl_probs.rds")
```


```{r plot-smpl-prob, eval=F}
pgrid <- expand.grid(method = names(methods)[3:4], data=names(data_sets))
dt_plot <- rbindlist(
  lapply(
    1:nrow(pgrid),
    function(i) {
      other_method <- as.character(pgrid$method[i])
      data_set <- pgrid$data[i]
      x <- smpl_probs[method=="BLEV" & data==data_set]$prob
      y <- smpl_probs[method==other_method & data==data_set]$prob
      data.table(x=x, y=y, y_var=other_method, data=data_set)
    }
  )
)
ggplot(dt_plot, aes(x=x, y=y)) +
  geom_point() +
  facet_wrap(
    y_var ~ data,
    scales = "free",
    ncol=length(data_sets)
  ) +
  labs(
    x="BLEV",
    y=""
  )
```

```{r, fig.height=4, fig.width=10}
ggplot(
  data = smpl_probs,
  aes(x=data, y=log(prob)) 
) +
  geom_boxplot() +
  facet_grid(
    col=vars(method)
  )
```

```{r}
# Simulation parameters:
m <- 10+round(sapply(1:7,function(i) (2^(i)*1e-3)) * n)
weighted <- c(T,F)
pgrid <- data.table(
  expand.grid(
    m=m, 
    method=names(methods), 
    weighted=weighted, 
    data_set=c("TT","MG","LN")
  )
)
```

```{r,eval=F}
set.seed(111)
grid_search <- rbindlist(
  lapply(
    1:nrow(pgrid),
    function(i) {
      m <- pgrid[i,m]
      estimator_name <- pgrid[i,method] 
      estimator <- methods[[estimator_name]]
      weighted <- pgrid[i, weighted]
      data_set <- pgrid$data[i]
      X <- data_sets[[data_set]]$X
      y <- data_sets[[data_set]]$y
      output <- sim_subsampling(X, y, m, estimator, weighted=weighted)
      output[,m:=m]
      output[,method:=estimator_name]
      output[,weighted:=weighted]
      output[,data_set:=data_set]
    }
  )
)
grid_search[,weighted:=factor(weighted)]
levels(grid_search$weighted) <- c("OLS","WLS")
grid_search[,log_value:=log(value)]
saveRDS(grid_search, file="outputs/grid_search_zhu.rds")
```

```{r}
grid_search <- readRDS("outputs/grid_search_zhu.rds")
p_list <- lapply(
  1:2,
  function(i) {
    ggplot(
      data=grid_search[weighted==levels(weighted)[i]], 
      aes(x=m/n, y=log_value, colour=method)
    ) +
      geom_line() +
      geom_point() +
      facet_wrap(
        variables ~ data_set,
        scales = "free",
        nrow=3
      ) +
      scale_color_discrete(
        name="Subsampling method:"
      ) +
      labs(
        x="m/n",
        y="Logarithm",
        title=levels(grid_search$weighted)[i]
      ) 
  }
)
names(p_list) <- levels(grid_search$weighted)
```

### Applied to sinusoidal function

```{r}
# Function parameters:
n <- 1000
p <- 9
a <- 0
b <- 1
x <- seq(a,b,length.out = n)
y_true <- sinusoidal(x)
y <- sim_sinusoidal(n)
v <- 0.3
mu <- seq(a,b,length.out = p)
Phi <- cbind(
  rep(1,n),
  sapply(
    1:length(mu),
    function(p) {
      mu_p <- mu[p]
      gauss_kernel(x=x, mu=mu_p, s = s)
    }
  )
)
# Simulation parameters:
m <- c(0.05,0.10,0.15,0.20,0.25,0.50,0.75) * n
weighted <- c(T,F)
param_grid <- data.table(expand.grid(m=m, method=names(methods), weighted=weighted))
```



```{r, eval=T}
set.seed(111)
grid_search <- rbindlist(
  lapply(
    1:nrow(param_grid),
    function(i) {
      m <- param_grid[i,m]
      estimator_name <- param_grid[i,method] 
      estimator <- methods[[estimator_name]]
      weighted <- param_grid[i, weighted]
      output <- sim_subsampling(Phi, y, m, estimator, weighted=weighted)
      output[,m:=m]
      output[,method:=estimator_name]
      output[,weighted:=weighted]
    }
  )
)
grid_search[,weighted:=factor(weighted)]
levels(grid_search$weighted) <- c("OLS","WLS")
grid_search[,log_value:=log(value)]
saveRDS(grid_search, file="outputs/grid_search.rds")
```

```{r}
grid_search <- readRDS("outputs/grid_search.rds")
ggplot(
  data=grid_search, 
  aes(x=m/n, y=log_value, colour=method)
) +
  geom_line() +
  geom_point() +
  facet_grid(
    rows=vars(variables),
    cols = vars(weighted)
  ) +
  scale_color_discrete(
    name="Subsampling method:"
  ) +
  labs(
    x="m/n",
    y="Logarithm"
  ) 
```



## References {-}
